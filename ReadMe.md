
# Daniel's Neural Network Adventures ReadMe

### Overview

The purpose of this GitHub repository is to document my adventures in building my first neural network. My goal is to build a simple multi-layered network and train it using stochastic gradient descent. I will build out all of the R code by hand to get a better appreciation for the mechanics of neural networks. 

### The Plan

1. Find and prepare a data set to work on
2. Build a single neuron and figure out how to train it
3. Chain together a few neurons by hand, train them, and see if they perform better than the single neuron
4. Created an automated way to chain together neurons

### Bonus Goals

1. Experiment with different types of training approaches and cost functions
2. Automatically create "random" networks, train them, and ensemble them to create a meta-model
  + Random activation functions
  + Random cost functions
  + Random connections
  
### Resources

[Hacker's guide to Neural Networks by Andrej Karpathy](http://karpathy.github.io/neuralnets/)

[A Step by Step Backpropagation Example by Matt Mazur](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)

[Neural Networks and Deep Learning by Michael Nielsen](http://neuralnetworksanddeeplearning.com/)

Regardless of my success with this project, I want to thank the above authors for their hard work. I'm also extending this thanks to the countless resources I will use that won't be mentioned here. Any failure here is my own. These guides are well written and I'm happy that I have access to such material for free.

Thanks for browsing!

Daniel Alaiev