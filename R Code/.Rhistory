# Regularization and learning for network
learning_rate <- 1/5e2
grad_reg <- 1/1e2
iter <- 1e4
# Creating sample index vector outside loop for speed and setting up
# default bias weight
samp_ind <- sample(1:nrow(all.dat$training), size = iter, replace = T)
bias <- 1
names(bias) <- "bias"
# Loop begins
for(i in 1:iter) {
# Compute partial derivatives of sigmoid function given weights and
# holding the inputs constant, compact form for speed
pdevs <- do.call(sig_der, as.list(c(weights,
bias,
inputs[samp_ind[i],])))
attr(pdevs,"gradient") <- attr(pdevs,"gradient")[-(1:outcome_pos)]
# Observed             # Predicted output at current input levels
pull <- outputs[samp_ind[i]] - head(pdevs)
# Taking the inverse of the partial derivatives to solve for change in weight
grad_inv <- 1/attr(pdevs, 'gradient')
# Apply gradient shrinkage, set a ceiling and floor
grad_inv <- sapply(grad_inv, function(grad) {
# Regularizes gradient
if(grad > grad_reg) return(grad_reg)
if(grad < -grad_reg) return(-grad_reg)
return(grad)
})
# Apply learning rate and change weights according to capped partial derivs
weights <- weights + (pull * grad_inv * learning_rate)
}
# Pushing out predictions
pred_gen <- function(inputs_df) {
preds <- vector("numeric", length = nrow(inputs_df))
dat <- data.frame(bias = 1,
inputs_df[,-outcome_pos])
for(i in 1:length(preds)) {
preds[i] <- head(do.call(sig_der, as.list(c(weights,
dat[i,]))))
}
return(preds)
}
preds <- pred_gen(all.dat$validation)
lib_load("MLmetrics")
# Log loss
# 0.1456651 #
LogLoss(y_pred = preds, y_true = all.dat$validation$Occupancy)
# Accuracy
lib_load("caret")
# 0.9786 # Looks like this is peak accuracy
round(confusionMatrix(data = round(preds,0),
reference = all.dat$validation$Occupancy)$overall,4)
learning_rate <- 1/1e2
grad_reg <- 1/1e2
iter <- 1e4
# Creating sample index vector outside loop for speed and setting up
# default bias weight
samp_ind <- sample(1:nrow(all.dat$training), size = iter, replace = T)
bias <- 1
names(bias) <- "bias"
# Loop begins
for(i in 1:iter) {
# Compute partial derivatives of sigmoid function given weights and
# holding the inputs constant, compact form for speed
pdevs <- do.call(sig_der, as.list(c(weights,
bias,
inputs[samp_ind[i],])))
attr(pdevs,"gradient") <- attr(pdevs,"gradient")[-(1:outcome_pos)]
# Observed             # Predicted output at current input levels
pull <- outputs[samp_ind[i]] - head(pdevs)
# Taking the inverse of the partial derivatives to solve for change in weight
grad_inv <- 1/attr(pdevs, 'gradient')
# Apply gradient shrinkage, set a ceiling and floor
grad_inv <- sapply(grad_inv, function(grad) {
# Regularizes gradient
if(grad > grad_reg) return(grad_reg)
if(grad < -grad_reg) return(-grad_reg)
return(grad)
})
# Apply learning rate and change weights according to capped partial derivs
weights <- weights + (pull * grad_inv * learning_rate)
}
# Pushing out predictions
pred_gen <- function(inputs_df) {
preds <- vector("numeric", length = nrow(inputs_df))
dat <- data.frame(bias = 1,
inputs_df[,-outcome_pos])
for(i in 1:length(preds)) {
preds[i] <- head(do.call(sig_der, as.list(c(weights,
dat[i,]))))
}
return(preds)
}
preds <- pred_gen(all.dat$validation)
lib_load("MLmetrics")
# Log loss
# 0.1456651 #
LogLoss(y_pred = preds, y_true = all.dat$validation$Occupancy)
# Accuracy
lib_load("caret")
# 0.9786 # Looks like this is peak accuracy
round(confusionMatrix(data = round(preds,0),
reference = all.dat$validation$Occupancy)$overall,4)
learning_rate <- 1/1e2
grad_reg <- 1/1e2
iter <- 1e4
# Creating sample index vector outside loop for speed and setting up
# default bias weight
samp_ind <- sample(1:nrow(all.dat$training), size = iter, replace = T)
bias <- 1
names(bias) <- "bias"
# Loop begins
for(i in 1:iter) {
# Compute partial derivatives of sigmoid function given weights and
# holding the inputs constant, compact form for speed
pdevs <- do.call(sig_der, as.list(c(weights,
bias,
inputs[samp_ind[i],])))
attr(pdevs,"gradient") <- attr(pdevs,"gradient")[-(1:outcome_pos)]
# Observed             # Predicted output at current input levels
pull <- outputs[samp_ind[i]] - head(pdevs)
# Taking the inverse of the partial derivatives to solve for change in weight
grad_inv <- 1/attr(pdevs, 'gradient')
# Apply gradient shrinkage, set a ceiling and floor
grad_inv <- sapply(grad_inv, function(grad) {
# Regularizes gradient
if(grad > grad_reg) return(grad_reg)
if(grad < -grad_reg) return(-grad_reg)
return(grad)
})
# Apply learning rate and change weights according to capped partial derivs
weights <- weights + (pull * grad_inv * learning_rate)
}
# Pushing out predictions
pred_gen <- function(inputs_df) {
preds <- vector("numeric", length = nrow(inputs_df))
dat <- data.frame(bias = 1,
inputs_df[,-outcome_pos])
for(i in 1:length(preds)) {
preds[i] <- head(do.call(sig_der, as.list(c(weights,
dat[i,]))))
}
return(preds)
}
preds <- pred_gen(all.dat$validation)
lib_load("MLmetrics")
# Log loss
# 0.1456651 #
LogLoss(y_pred = preds, y_true = all.dat$validation$Occupancy)
# Accuracy
lib_load("caret")
# 0.9786 # Looks like this is peak accuracy
round(confusionMatrix(data = round(preds,0),
reference = all.dat$validation$Occupancy)$overall,4)
learning_rate <- 1/1e2
grad_reg <- 1/1e2
iter <- 1e4
# Creating sample index vector outside loop for speed and setting up
# default bias weight
samp_ind <- sample(1:nrow(all.dat$training), size = iter, replace = T)
bias <- 1
names(bias) <- "bias"
# Loop begins
for(i in 1:iter) {
# Compute partial derivatives of sigmoid function given weights and
# holding the inputs constant, compact form for speed
pdevs <- do.call(sig_der, as.list(c(weights,
bias,
inputs[samp_ind[i],])))
attr(pdevs,"gradient") <- attr(pdevs,"gradient")[-(1:outcome_pos)]
# Observed             # Predicted output at current input levels
pull <- outputs[samp_ind[i]] - head(pdevs)
# Taking the inverse of the partial derivatives to solve for change in weight
grad_inv <- 1/attr(pdevs, 'gradient')
# Apply gradient shrinkage, set a ceiling and floor
grad_inv <- sapply(grad_inv, function(grad) {
# Regularizes gradient
if(grad > grad_reg) return(grad_reg)
if(grad < -grad_reg) return(-grad_reg)
return(grad)
})
# Apply learning rate and change weights according to capped partial derivs
weights <- weights + (pull * grad_inv * learning_rate)
}
# Pushing out predictions
pred_gen <- function(inputs_df) {
preds <- vector("numeric", length = nrow(inputs_df))
dat <- data.frame(bias = 1,
inputs_df[,-outcome_pos])
for(i in 1:length(preds)) {
preds[i] <- head(do.call(sig_der, as.list(c(weights,
dat[i,]))))
}
return(preds)
}
preds <- pred_gen(all.dat$validation)
lib_load("MLmetrics")
# Log loss
# 0.1456651 #
LogLoss(y_pred = preds, y_true = all.dat$validation$Occupancy)
# Accuracy
lib_load("caret")
# 0.9786 # Looks like this is peak accuracy
round(confusionMatrix(data = round(preds,0),
reference = all.dat$validation$Occupancy)$overall,4)
learning_rate <- 1/1e2
grad_reg <- 1/1e2
iter <- 1e4
# Creating sample index vector outside loop for speed and setting up
# default bias weight
samp_ind <- sample(1:nrow(all.dat$training), size = iter, replace = T)
bias <- 1
names(bias) <- "bias"
# Loop begins
for(i in 1:iter) {
# Compute partial derivatives of sigmoid function given weights and
# holding the inputs constant, compact form for speed
pdevs <- do.call(sig_der, as.list(c(weights,
bias,
inputs[samp_ind[i],])))
attr(pdevs,"gradient") <- attr(pdevs,"gradient")[-(1:outcome_pos)]
# Observed             # Predicted output at current input levels
pull <- outputs[samp_ind[i]] - head(pdevs)
# Taking the inverse of the partial derivatives to solve for change in weight
grad_inv <- 1/attr(pdevs, 'gradient')
# Apply gradient shrinkage, set a ceiling and floor
grad_inv <- sapply(grad_inv, function(grad) {
# Regularizes gradient
if(grad > grad_reg) return(grad_reg)
if(grad < -grad_reg) return(-grad_reg)
return(grad)
})
# Apply learning rate and change weights according to capped partial derivs
weights <- weights + (pull * grad_inv * learning_rate)
}
# Pushing out predictions
pred_gen <- function(inputs_df) {
preds <- vector("numeric", length = nrow(inputs_df))
dat <- data.frame(bias = 1,
inputs_df[,-outcome_pos])
for(i in 1:length(preds)) {
preds[i] <- head(do.call(sig_der, as.list(c(weights,
dat[i,]))))
}
return(preds)
}
preds <- pred_gen(all.dat$validation)
lib_load("MLmetrics")
# Log loss
# 0.1456651 #
LogLoss(y_pred = preds, y_true = all.dat$validation$Occupancy)
# Accuracy
lib_load("caret")
# 0.9786 # Looks like this is peak accuracy
round(confusionMatrix(data = round(preds,0),
reference = all.dat$validation$Occupancy)$overall,4)
learning_rate <- 1/1e2
grad_reg <- 1/1e2
iter <- 1e4
# Creating sample index vector outside loop for speed and setting up
# default bias weight
samp_ind <- sample(1:nrow(all.dat$training), size = iter, replace = T)
bias <- 1
names(bias) <- "bias"
# Loop begins
for(i in 1:iter) {
# Compute partial derivatives of sigmoid function given weights and
# holding the inputs constant, compact form for speed
pdevs <- do.call(sig_der, as.list(c(weights,
bias,
inputs[samp_ind[i],])))
attr(pdevs,"gradient") <- attr(pdevs,"gradient")[-(1:outcome_pos)]
# Observed             # Predicted output at current input levels
pull <- outputs[samp_ind[i]] - head(pdevs)
# Taking the inverse of the partial derivatives to solve for change in weight
grad_inv <- 1/attr(pdevs, 'gradient')
# Apply gradient shrinkage, set a ceiling and floor
grad_inv <- sapply(grad_inv, function(grad) {
# Regularizes gradient
if(grad > grad_reg) return(grad_reg)
if(grad < -grad_reg) return(-grad_reg)
return(grad)
})
# Apply learning rate and change weights according to capped partial derivs
weights <- weights + (pull * grad_inv * learning_rate)
}
# Pushing out predictions
pred_gen <- function(inputs_df) {
preds <- vector("numeric", length = nrow(inputs_df))
dat <- data.frame(bias = 1,
inputs_df[,-outcome_pos])
for(i in 1:length(preds)) {
preds[i] <- head(do.call(sig_der, as.list(c(weights,
dat[i,]))))
}
return(preds)
}
preds <- pred_gen(all.dat$validation)
lib_load("MLmetrics")
# Log loss
# 0.1456651 #
LogLoss(y_pred = preds, y_true = all.dat$validation$Occupancy)
# Accuracy
lib_load("caret")
# 0.9786 # Looks like this is peak accuracy
round(confusionMatrix(data = round(preds,0),
reference = all.dat$validation$Occupancy)$overall,4)
learning_rate <- 1/1e2
grad_reg <- 1/1e2
iter <- 1e4
# Creating sample index vector outside loop for speed and setting up
# default bias weight
samp_ind <- sample(1:nrow(all.dat$training), size = iter, replace = T)
bias <- 1
names(bias) <- "bias"
# Loop begins
for(i in 1:iter) {
# Compute partial derivatives of sigmoid function given weights and
# holding the inputs constant, compact form for speed
pdevs <- do.call(sig_der, as.list(c(weights,
bias,
inputs[samp_ind[i],])))
attr(pdevs,"gradient") <- attr(pdevs,"gradient")[-(1:outcome_pos)]
# Observed             # Predicted output at current input levels
pull <- outputs[samp_ind[i]] - head(pdevs)
# Taking the inverse of the partial derivatives to solve for change in weight
grad_inv <- 1/attr(pdevs, 'gradient')
# Apply gradient shrinkage, set a ceiling and floor
grad_inv <- sapply(grad_inv, function(grad) {
# Regularizes gradient
if(grad > grad_reg) return(grad_reg)
if(grad < -grad_reg) return(-grad_reg)
return(grad)
})
# Apply learning rate and change weights according to capped partial derivs
weights <- weights + (pull * grad_inv * learning_rate)
}
# Pushing out predictions
pred_gen <- function(inputs_df) {
preds <- vector("numeric", length = nrow(inputs_df))
dat <- data.frame(bias = 1,
inputs_df[,-outcome_pos])
for(i in 1:length(preds)) {
preds[i] <- head(do.call(sig_der, as.list(c(weights,
dat[i,]))))
}
return(preds)
}
preds <- pred_gen(all.dat$validation)
lib_load("MLmetrics")
# Log loss
# 0.1456651 #
LogLoss(y_pred = preds, y_true = all.dat$validation$Occupancy)
# Accuracy
lib_load("caret")
# 0.9786 # Looks like this is peak accuracy
round(confusionMatrix(data = round(preds,0),
reference = all.dat$validation$Occupancy)$overall,4)
learning_rate <- 1/1e2
grad_reg <- 1/1e2
iter <- 1e5
# Creating sample index vector outside loop for speed and setting up
# default bias weight
samp_ind <- sample(1:nrow(all.dat$training), size = iter, replace = T)
bias <- 1
names(bias) <- "bias"
# Loop begins
for(i in 1:iter) {
# Compute partial derivatives of sigmoid function given weights and
# holding the inputs constant, compact form for speed
pdevs <- do.call(sig_der, as.list(c(weights,
bias,
inputs[samp_ind[i],])))
attr(pdevs,"gradient") <- attr(pdevs,"gradient")[-(1:outcome_pos)]
# Observed             # Predicted output at current input levels
pull <- outputs[samp_ind[i]] - head(pdevs)
# Taking the inverse of the partial derivatives to solve for change in weight
grad_inv <- 1/attr(pdevs, 'gradient')
# Apply gradient shrinkage, set a ceiling and floor
grad_inv <- sapply(grad_inv, function(grad) {
# Regularizes gradient
if(grad > grad_reg) return(grad_reg)
if(grad < -grad_reg) return(-grad_reg)
return(grad)
})
# Apply learning rate and change weights according to capped partial derivs
weights <- weights + (pull * grad_inv * learning_rate)
}
# Pushing out predictions
pred_gen <- function(inputs_df) {
preds <- vector("numeric", length = nrow(inputs_df))
dat <- data.frame(bias = 1,
inputs_df[,-outcome_pos])
for(i in 1:length(preds)) {
preds[i] <- head(do.call(sig_der, as.list(c(weights,
dat[i,]))))
}
return(preds)
}
preds <- pred_gen(all.dat$validation)
lib_load("MLmetrics")
# Log loss
# 0.1456651 #
LogLoss(y_pred = preds, y_true = all.dat$validation$Occupancy)
# Accuracy
lib_load("caret")
# 0.9786 # Looks like this is peak accuracy
round(confusionMatrix(data = round(preds,0),
reference = all.dat$validation$Occupancy)$overall,4)
learning_rate <- 1/1e2
grad_reg <- 1/1e2
iter <- 1e5
# Creating sample index vector outside loop for speed and setting up
# default bias weight
samp_ind <- sample(1:nrow(all.dat$training), size = iter, replace = T)
bias <- 1
names(bias) <- "bias"
# Loop begins
for(i in 1:iter) {
# Compute partial derivatives of sigmoid function given weights and
# holding the inputs constant, compact form for speed
pdevs <- do.call(sig_der, as.list(c(weights,
bias,
inputs[samp_ind[i],])))
attr(pdevs,"gradient") <- attr(pdevs,"gradient")[-(1:outcome_pos)]
# Observed             # Predicted output at current input levels
pull <- outputs[samp_ind[i]] - head(pdevs)
# Taking the inverse of the partial derivatives to solve for change in weight
grad_inv <- 1/attr(pdevs, 'gradient')
# Apply gradient shrinkage, set a ceiling and floor
grad_inv <- sapply(grad_inv, function(grad) {
# Regularizes gradient
if(grad > grad_reg) return(grad_reg)
if(grad < -grad_reg) return(-grad_reg)
return(grad)
})
# Apply learning rate and change weights according to capped partial derivs
weights <- weights + (pull * grad_inv * learning_rate)
}
# Pushing out predictions
pred_gen <- function(inputs_df) {
preds <- vector("numeric", length = nrow(inputs_df))
dat <- data.frame(bias = 1,
inputs_df[,-outcome_pos])
for(i in 1:length(preds)) {
preds[i] <- head(do.call(sig_der, as.list(c(weights,
dat[i,]))))
}
return(preds)
}
preds <- pred_gen(all.dat$validation)
lib_load("MLmetrics")
# Log loss
# 0.1456651 #
LogLoss(y_pred = preds, y_true = all.dat$validation$Occupancy)
# Accuracy
lib_load("caret")
# 0.9786 # Looks like this is peak accuracy
round(confusionMatrix(data = round(preds,0),
reference = all.dat$validation$Occupancy)$overall,4)
learning_rate <- 1/1e2
grad_reg <- 1/1e2
iter <- 1e6
# Creating sample index vector outside loop for speed and setting up
# default bias weight
samp_ind <- sample(1:nrow(all.dat$training), size = iter, replace = T)
bias <- 1
names(bias) <- "bias"
# Loop begins
for(i in 1:iter) {
# Compute partial derivatives of sigmoid function given weights and
# holding the inputs constant, compact form for speed
pdevs <- do.call(sig_der, as.list(c(weights,
bias,
inputs[samp_ind[i],])))
attr(pdevs,"gradient") <- attr(pdevs,"gradient")[-(1:outcome_pos)]
# Observed             # Predicted output at current input levels
pull <- outputs[samp_ind[i]] - head(pdevs)
# Taking the inverse of the partial derivatives to solve for change in weight
grad_inv <- 1/attr(pdevs, 'gradient')
# Apply gradient shrinkage, set a ceiling and floor
grad_inv <- sapply(grad_inv, function(grad) {
# Regularizes gradient
if(grad > grad_reg) return(grad_reg)
if(grad < -grad_reg) return(-grad_reg)
return(grad)
})
# Apply learning rate and change weights according to capped partial derivs
weights <- weights + (pull * grad_inv * learning_rate)
}
# Pushing out predictions
pred_gen <- function(inputs_df) {
preds <- vector("numeric", length = nrow(inputs_df))
dat <- data.frame(bias = 1,
inputs_df[,-outcome_pos])
for(i in 1:length(preds)) {
preds[i] <- head(do.call(sig_der, as.list(c(weights,
dat[i,]))))
}
return(preds)
}
preds <- pred_gen(all.dat$validation)
lib_load("MLmetrics")
# Log loss
# 0.1456651 #
LogLoss(y_pred = preds, y_true = all.dat$validation$Occupancy)
# Accuracy
lib_load("caret")
# 0.9786 # Looks like this is peak accuracy
round(confusionMatrix(data = round(preds,0),
reference = all.dat$validation$Occupancy)$overall,4)
save.image()
