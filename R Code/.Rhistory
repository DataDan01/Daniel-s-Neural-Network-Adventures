for(j in 1:neurons) {
# Replace each known node with its underlying function
for(q in 1:neurons) {
net[[i]][j] <- gsub(pattern = names(net[[i-1]][q]),
replacement = net[[i-1]][q],
x = net[[i]][j])
}
}
# Clear out previous layer to save memory
net[[i]] <- rep(NULL, neurons)
}
# Roll into giant final function and into sigmoid
# Only source data preparation script if data are missing
if(!("all.dat" %in% ls())) source("./DataPrep.R")
# Creating a deep network:
# 1) Specifiy number of layers and neurons per layer,
# minimum number of connections
# 2) Expand list of neurons and randomly connect them
# 3) Fill in lists with functions and weights
# 4) Roll everything up in sigmoid
# 5) Differentiate and compile
# 6) Train and test
# Initializing parameters and empty network
layers <- 4
neurons <- 5
min_conn <- 3
net <- vector(mode = 'list', length = layers+1)
# Setting up input layer
net[[1]] <- colnames(all.dat$training)[1:ncol(all.dat$training)-1]
names(net[[1]]) <- net[[1]]
# Layer first (i), then neuron number in layer (j)
# Naming network and sampling to find number of connections
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
net[[i]][j] <- sample(min_conn:neurons, 1)
names(net[[i]])[j] <- paste0("n_",i,"_",j)
}
}
# Filling in layers with previous layers and adding weights
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
# Sampling previous layer under constraints specified above
inputs <- sample(x = names(net[[i-1]]),
size = net[[i]][j],
replace = F)
# Appending weights to layer's inputs
w_inputs <- sapply(inputs, function(input) {
paste0(input,
" * ",
paste0("w(",i,j,")_",input))
})
# Inserting into network
net[[i]][j] <- paste0(w_inputs, collapse = " + ")
# Adding bias
net[[i]][j] <- paste0(net[[i]][j], " + ",
paste0("w(",i,j,")_bias * bias"))
# Closing with parens
net[[i]][j] <- paste0("(",net[[i]][j],")")
}
}
# Wrapping functions around nodes
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
net[[i]][j] <- paste0("(log(1+exp(1)^",net[[i]][j],"))")
}
}
i = 1
i = 3
for(j in 1:neurons) {
# Replace each known node with its underlying function
for(q in 1:neurons) {
net[[i]][j] <- gsub(pattern = names(net[[i-1]][q]),
replacement = net[[i-1]][q],
x = net[[i]][j])
}
}
net[[3]][1]
net[[3]][2]
rep(NULL, neurons)
rep("", neurons)
net[[i-1]] <- ""
i = 4
# Only source data preparation script if data are missing
if(!("all.dat" %in% ls())) source("./DataPrep.R")
# Creating a deep network:
# 1) Specifiy number of layers and neurons per layer,
# minimum number of connections
# 2) Expand list of neurons and randomly connect them
# 3) Fill in lists with functions and weights
# 4) Roll everything up in sigmoid
# 5) Differentiate and compile
# 6) Train and test
# Initializing parameters and empty network
layers <- 4
neurons <- 5
min_conn <- 3
net <- vector(mode = 'list', length = layers+1)
# Setting up input layer
net[[1]] <- colnames(all.dat$training)[1:ncol(all.dat$training)-1]
names(net[[1]]) <- net[[1]]
# Layer first (i), then neuron number in layer (j)
# Naming network and sampling to find number of connections
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
net[[i]][j] <- sample(min_conn:neurons, 1)
names(net[[i]])[j] <- paste0("n_",i,"_",j)
}
}
# Filling in layers with previous layers and adding weights
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
# Sampling previous layer under constraints specified above
inputs <- sample(x = names(net[[i-1]]),
size = net[[i]][j],
replace = F)
# Appending weights to layer's inputs
w_inputs <- sapply(inputs, function(input) {
paste0(input,
" * ",
paste0("w(",i,j,")_",input))
})
# Inserting into network
net[[i]][j] <- paste0(w_inputs, collapse = " + ")
# Adding bias
net[[i]][j] <- paste0(net[[i]][j], " + ",
paste0("w(",i,j,")_bias * bias"))
# Closing with parens
net[[i]][j] <- paste0("(",net[[i]][j],")")
}
}
# Wrapping functions around nodes
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
net[[i]][j] <- paste0("(log(1+exp(1)^",net[[i]][j],"))")
}
}
# Substitute bottom into layers into next layers
for(i in 3:(layers)) {
# For each neuron
for(j in 1:neurons) {
# Replace each known node with its underlying function
for(q in 1:neurons) {
net[[i]][j] <- gsub(pattern = names(net[[i-1]][q]),
replacement = net[[i-1]][q],
x = net[[i]][j])
}
}
# Clear out previous layer to save memory
net[[i-1]] <- ""
}
# Roll into giant final function and into sigmoid
# Only source data preparation script if data are missing
if(!("all.dat" %in% ls())) source("./DataPrep.R")
# Creating a deep network:
# 1) Specifiy number of layers and neurons per layer,
# minimum number of connections
# 2) Expand list of neurons and randomly connect them
# 3) Fill in lists with functions and weights
# 4) Roll everything up in sigmoid
# 5) Differentiate and compile
# 6) Train and test
# Initializing parameters and empty network
layers <- 4
neurons <- 5
min_conn <- 3
net <- vector(mode = 'list', length = layers+1)
# Setting up input layer
net[[1]] <- colnames(all.dat$training)[1:ncol(all.dat$training)-1]
names(net[[1]]) <- net[[1]]
# Layer first (i), then neuron number in layer (j)
# Naming network and sampling to find number of connections
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
net[[i]][j] <- sample(min_conn:neurons, 1)
names(net[[i]])[j] <- paste0("n_",i,"_",j)
}
}
# Filling in layers with previous layers and adding weights
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
# Sampling previous layer under constraints specified above
inputs <- sample(x = names(net[[i-1]]),
size = net[[i]][j],
replace = F)
# Appending weights to layer's inputs
w_inputs <- sapply(inputs, function(input) {
paste0(input,
" * ",
paste0("w(",i,j,")_",input))
})
# Inserting into network
net[[i]][j] <- paste0(w_inputs, collapse = " + ")
# Adding bias
net[[i]][j] <- paste0(net[[i]][j], " + ",
paste0("w(",i,j,")_bias * bias"))
# Closing with parens
net[[i]][j] <- paste0("(",net[[i]][j],")")
}
}
# Wrapping functions around nodes
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
net[[i]][j] <- paste0("(log(1+exp(1)^",net[[i]][j],"))")
}
}
# Substitute bottom into layers into next layers
for(i in 3:(layers+1)) {
# For each neuron
for(j in 1:neurons) {
# Replace each known node with its underlying function
for(q in 1:neurons) {
net[[i]][j] <- gsub(pattern = names(net[[i-1]][q]),
replacement = net[[i-1]][q],
x = net[[i]][j])
}
}
# Clear out previous layer to save memory
net[[i-1]] <- ""
}
# Roll into giant final function and into sigmoid
# Only source data preparation script if data are missing
if(!("all.dat" %in% ls())) source("./DataPrep.R")
# Creating a deep network:
# 1) Specifiy number of layers and neurons per layer,
# minimum number of connections
# 2) Expand list of neurons and randomly connect them
# 3) Fill in lists with functions and weights
# 4) Roll everything up in sigmoid
# 5) Differentiate and compile
# 6) Train and test
# Initializing parameters and empty network
layers <- 7
neurons <- 5
min_conn <- 3
net <- vector(mode = 'list', length = layers+1)
# Setting up input layer
net[[1]] <- colnames(all.dat$training)[1:ncol(all.dat$training)-1]
names(net[[1]]) <- net[[1]]
# Layer first (i), then neuron number in layer (j)
# Naming network and sampling to find number of connections
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
net[[i]][j] <- sample(min_conn:neurons, 1)
names(net[[i]])[j] <- paste0("n_",i,"_",j)
}
}
# Filling in layers with previous layers and adding weights
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
# Sampling previous layer under constraints specified above
inputs <- sample(x = names(net[[i-1]]),
size = net[[i]][j],
replace = F)
# Appending weights to layer's inputs
w_inputs <- sapply(inputs, function(input) {
paste0(input,
" * ",
paste0("w(",i,j,")_",input))
})
# Inserting into network
net[[i]][j] <- paste0(w_inputs, collapse = " + ")
# Adding bias
net[[i]][j] <- paste0(net[[i]][j], " + ",
paste0("w(",i,j,")_bias * bias"))
# Closing with parens
net[[i]][j] <- paste0("(",net[[i]][j],")")
}
}
# Wrapping functions around nodes
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
net[[i]][j] <- paste0("(log(1+exp(1)^",net[[i]][j],"))")
}
}
# Substitute bottom into layers into next layers
for(i in 3:(layers+1)) {
# For each neuron
for(j in 1:neurons) {
# Replace each known node with its underlying function
for(q in 1:neurons) {
net[[i]][j] <- gsub(pattern = names(net[[i-1]][q]),
replacement = net[[i-1]][q],
x = net[[i]][j])
}
}
# Clear out previous layer to save memory
net[[i-1]] <- ""
}
# Roll into giant final function and into sigmoid
# Only source data preparation script if data are missing
if(!("all.dat" %in% ls())) source("./DataPrep.R")
# Creating a deep network:
# 1) Specifiy number of layers and neurons per layer,
# minimum number of connections
# 2) Expand list of neurons and randomly connect them
# 3) Fill in lists with functions and weights
# 4) Roll everything up in sigmoid
# 5) Differentiate and compile
# 6) Train and test
# Initializing parameters and empty network
layers <- 7
neurons <- 5
min_conn <- 3
net <- vector(mode = 'list', length = layers+1)
# Setting up input layer
net[[1]] <- colnames(all.dat$training)[1:ncol(all.dat$training)-1]
names(net[[1]]) <- net[[1]]
# Layer first (i), then neuron number in layer (j)
# Naming network and sampling to find number of connections
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
net[[i]][j] <- sample(min_conn:neurons, 1)
names(net[[i]])[j] <- paste0("n_",i,"_",j)
}
}
# Filling in layers with previous layers and adding weights
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
# Sampling previous layer under constraints specified above
inputs <- sample(x = names(net[[i-1]]),
size = net[[i]][j],
replace = F)
# Appending weights to layer's inputs
w_inputs <- sapply(inputs, function(input) {
paste0(input,
" * ",
paste0("w(",i,j,")_",input))
})
# Inserting into network
net[[i]][j] <- paste0(w_inputs, collapse = " + ")
# Adding bias
net[[i]][j] <- paste0(net[[i]][j], " + ",
paste0("w(",i,j,")_bias * bias"))
# Closing with parens
net[[i]][j] <- paste0("(",net[[i]][j],")")
}
}
# Wrapping functions around nodes
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
net[[i]][j] <- paste0("(log(1+exp(1)^",net[[i]][j],"))")
}
}
# Substitute bottom into layers into next layers
for(i in 3:(layers+1)) {
# For each neuron
for(j in 1:neurons) {
# Replace each known node with its underlying function
for(q in 1:neurons) {
net[[i]][j] <- gsub(pattern = names(net[[i-1]][q]),
replacement = net[[i-1]][q],
x = net[[i]][j])
}
}
# Clear out previous layer to save memory
#net[[i-1]] <- ""
}
# Roll into giant final function and into sigmoid
# Only source data preparation script if data are missing
if(!("all.dat" %in% ls())) source("./DataPrep.R")
# Creating a deep network:
# 1) Specifiy number of layers and neurons per layer,
# minimum number of connections
# 2) Expand list of neurons and randomly connect them
# 3) Fill in lists with functions and weights
# 4) Roll everything up in sigmoid
# 5) Differentiate and compile
# 6) Train and test
# Initializing parameters and empty network
layers <- 5
neurons <- 5
min_conn <- 3
net <- vector(mode = 'list', length = layers+1)
# Setting up input layer
net[[1]] <- colnames(all.dat$training)[1:ncol(all.dat$training)-1]
names(net[[1]]) <- net[[1]]
# Layer first (i), then neuron number in layer (j)
# Naming network and sampling to find number of connections
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
net[[i]][j] <- sample(min_conn:neurons, 1)
names(net[[i]])[j] <- paste0("n_",i,"_",j)
}
}
# Filling in layers with previous layers and adding weights
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
# Sampling previous layer under constraints specified above
inputs <- sample(x = names(net[[i-1]]),
size = net[[i]][j],
replace = F)
# Appending weights to layer's inputs
w_inputs <- sapply(inputs, function(input) {
paste0(input,
" * ",
paste0("w(",i,j,")_",input))
})
# Inserting into network
net[[i]][j] <- paste0(w_inputs, collapse = " + ")
# Adding bias
net[[i]][j] <- paste0(net[[i]][j], " + ",
paste0("w(",i,j,")_bias * bias"))
# Closing with parens
net[[i]][j] <- paste0("(",net[[i]][j],")")
}
}
# Wrapping functions around nodes
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
net[[i]][j] <- paste0("(log(1+exp(1)^",net[[i]][j],"))")
}
}
# Substitute bottom into layers into next layers
for(i in 3:(layers+1)) {
# For each neuron
for(j in 1:neurons) {
# Replace each known node with its underlying function
for(q in 1:neurons) {
net[[i]][j] <- gsub(pattern = names(net[[i-1]][q]),
replacement = net[[i-1]][q],
x = net[[i]][j])
}
}
# Clear out previous layer to save memory
net[[i-1]] <- ""
}
# Roll into giant final function and into sigmoid
sig_input <- paste0("(",net[[layers+1]],")")
# Only source data preparation script if data are missing
if(!("all.dat" %in% ls())) source("./DataPrep.R")
# Creating a deep network:
# 1) Specifiy number of layers and neurons per layer,
# minimum number of connections
# 2) Expand list of neurons and randomly connect them
# 3) Fill in lists with functions and weights
# 4) Roll everything up in sigmoid
# 5) Differentiate and compile
# 6) Train and test
# Initializing parameters and empty network
layers <- 2
neurons <- 3
min_conn <- 3
net <- vector(mode = 'list', length = layers+1)
# Setting up input layer
net[[1]] <- colnames(all.dat$training)[1:ncol(all.dat$training)-1]
names(net[[1]]) <- net[[1]]
# Layer first (i), then neuron number in layer (j)
# Naming network and sampling to find number of connections
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
net[[i]][j] <- sample(min_conn:neurons, 1)
names(net[[i]])[j] <- paste0("n_",i,"_",j)
}
}
# Filling in layers with previous layers and adding weights
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
# Sampling previous layer under constraints specified above
inputs <- sample(x = names(net[[i-1]]),
size = net[[i]][j],
replace = F)
# Appending weights to layer's inputs
w_inputs <- sapply(inputs, function(input) {
paste0(input,
" * ",
paste0("w(",i,j,")_",input))
})
# Inserting into network
net[[i]][j] <- paste0(w_inputs, collapse = " + ")
# Adding bias
net[[i]][j] <- paste0(net[[i]][j], " + ",
paste0("w(",i,j,")_bias * bias"))
# Closing with parens
net[[i]][j] <- paste0("(",net[[i]][j],")")
}
}
# Wrapping functions around nodes
for(i in 2:(layers+1)) {
for(j in 1:neurons) {
net[[i]][j] <- paste0("(log(1+exp(1)^",net[[i]][j],"))")
}
}
# Substitute bottom into layers into next layers
for(i in 3:(layers+1)) {
# For each neuron
for(j in 1:neurons) {
# Replace each known node with its underlying function
for(q in 1:neurons) {
net[[i]][j] <- gsub(pattern = names(net[[i-1]][q]),
replacement = net[[i-1]][q],
x = net[[i]][j])
}
}
# Clear out previous layer to save memory
net[[i-1]] <- ""
}
# Roll into giant final function and into sigmoid
sig_input <- paste0("(",net[[layers+1]],")", collapse = " + ")
sig_input
paste0(c("1","2","3"), collapse = " + ")
sig_input <- paste0("(",
paste0(net[[layers+1]], collapse = " + "),
")"
)
sig_input
rm(net)
sig_fun <- paste0("1/(1+exp(1)^(",sig_input,"))")
rm(sig_fun)
grep(pattern = "w(", sig_fun)
sig_fun <- paste0("1/(1+exp(1)^(",sig_input,"))")
grep(pattern = "w(", sig_fun)
sig_fun
